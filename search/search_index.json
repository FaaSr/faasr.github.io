{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Overview","text":"<p>FaaSr is serverless Function-as-a-Service cloud software that makes it easy for developers to create functions and compose them in workflow graphs that can run unattended and on-demand</p> <p>FaaSr simplifies deploying reproducible FaaS workflows in different cloud providers without any code changes, with a primary use case in scientific workflows</p>"},{"location":"#cloud-platforms","title":"Cloud platforms","text":"<p>FaaSr supports the following cloud computing platforms:</p> <ul> <li>GitHub Actions - free-tier resources for small-scale workflows</li> <li>AWS Lambda - commercial cloud for scalable, low-latency worrkflows</li> <li>Google Cloud Run - commercial cloud for resource-intensive workflows</li> <li>OpenWhisk - open-source FaaS platform for private clouds</li> <li>Slurm - open-source job schedulers for private HPC clusters</li> </ul> <p>and the S3 protocol for cloud storage in platforms including:</p> <ul> <li>Minio - open-source S3 software</li> <li>AWS S3 - commercial S3 provider</li> <li>OSN - academic S3 provider in the USA</li> </ul>"},{"location":"#programming-languages","title":"Programming languages","text":"<p>FaaSr supports workflows with functions written in Python, R, or a combination of both</p>"},{"location":"#requirements","title":"Requirements","text":"<p>To get started with FaaSr you need:</p> <ul> <li>A GitHub account and associated credentials (a PAT token)</li> <li>An S3 data store bucket and associated credentials (access and secret keys)</li> <li>A FaaSr-workflow GitHub repository in your account</li> <li>A workflow configuration JSON file stored in your account's FaaSr-workflow repository</li> <li>A function code repository</li> </ul>"},{"location":"#getting-started","title":"Getting started","text":"<p>It's often easier to learn by doing: read the workflow model introduction for a brief overview of how FaaSr workflows are composed, and then the FaaSr tutorial guides you through setting up your workflow repo using a freely available public S3 test bucket hosted by Minio Play. Once you are comfortable with the initial setup from the tutorial, you will be able to:</p> <ul> <li>Configure additional cloud compute and data storage accounts</li> <li>Create your own functions using FaaSr R APIs and/or FaaSr Python APIs</li> <li>Configure your own workflows using the FaaSr Workflow Builder Web UI that produces FaaSr-compliant JSON configurations</li> <li>Register your own workflows with one or more cloud providers</li> <li>Invoke your workflows</li> <li>Verify and debug your workflow logs</li> </ul>"},{"location":"about/","title":"About FaaSr","text":"<p>FaaSr is research software that evolved from the experience with FLARE to become a generally-applicable FaaS serverless middleware for R and Python functions</p> <p>The FaaSr software follows the MIT open-source license is funded in part by a grant from the National Science Foundation (OAC-2450241 and OAC-2311124)). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.</p> <p>You can help us show our impact to our sponsors and other users by linking to this Web site, and citing our software in publications that have resulted from the use of the software. If your project uses the software, please consider adding a \"Powered by FaaSr\" to your Web site, linking back to faasr.io</p> <p>If you publish a research paper that has leveraged the software, please include as references:</p> <ul> <li>'Sungjae Park, R. Quinn Thomas, Cayelan C. Carey, Austin D. Delany, Yun-Jung Ku, Mary E. Lofton, Renato J. Figueiredo, \u201cFaaSr: Cross-Platform Function-as-a-Service Serverless Scientific Workflows in R\u201d, 20th International IEEE eScience Conference, 2024'</li> <li>'Sungjae Park, Yun-Jung Ku, Nan Mu, Vahid Daneshmand, R. Quinn Thomas, Cayelan C. Carey, Renato J. Figueiredo, \u201cFaaSr: R Package for Function-as-a-Service Cloud Computing\u201d, Journal of Open Source Software, 9(103)'</li> </ul>"},{"location":"advanced/","title":"Advanced usage","text":""},{"location":"advanced/#creating-custom-container-images","title":"Creating custom container images","text":""},{"location":"advanced/#custom-compute-server-names-and-registerinvoke-actions","title":"Custom compute server names and register/invoke actions","text":""},{"location":"community/","title":"Community","text":"<p>The goal of FaaSr is to enable research and development in event-driven Function-as-a-Service workflows for science domains including (but not limited to) natural sciences. If you are a researcher interested in using the software for your project, we would love to hear from you - please contact the team and join the user's group to let us know more about how we can help, and if you'd like your project to be highlighted in this page.</p> <p>If you publish research results that use the software, please make sure you cite our software in your publications to help us show the impact of our software to our sponsors.</p>"},{"location":"community/#users-and-practitioners","title":"Users and practitioners","text":"<p>If you are a user, or potential user of the software, please join our user's group and connect with the team and other users.</p>"},{"location":"community/#code-developers","title":"Code developers","text":"<p>We are excited to bring additional developers to contribute code, as well as contributions in documentation, examples, vignettes, supporting additional FaaS platforms, and others.</p> <p>Our software repository is on GitHub, which many developers are familiar with. We follow a review process \u2013 pull requests are thoroughly reviewed by one or more experienced peer developers before being incorporated in the code base. Therefore, we expect code of good quality that is thoroughly tested before a pull request is accepted. We provide integration tests that allow developers to test their code before a pull request.</p> <p>You should feel free to fork our repositories and work on your own fork; if you would like to commit code to our branch, the actual steps of this process are outlined in a separate document. Contributors interested in embarking on implementing a project that may require significant changes to software modules are encouraged to contact us before doing so to make sure we understand the scope of the project.</p>"},{"location":"conditional/","title":"Conditional Invocation","text":"<p>FaaSr supports the conditional invocation of actions in the workflow. This is useful in workflows where actions may or may not execute depending on the return value (true or false) of a predecessor.</p>"},{"location":"conditional/#expressing-conditional-invocation-in-the-workflow","title":"Expressing conditional invocation in the workflow","text":"<p>Consider the example below, where the action <code>DownloadData</code> always invokes <code>TestData</code> unconditionally (black edges denote unconditional execution). Now suppose <code>TestData</code> invokes a user function (in R or Python) <code>my_test()</code> that returns either <code>True</code> or <code>False</code>. </p> <p>Then, depending on the return value of <code>my_test()</code>, either <code>ComputeT</code> is invoked (return value <code>True</code>, green edge), or <code>ComputeF</code> is invoked (return value <code>False</code>, red edge) - but not both.</p> <p></p> <p>Finally, the last action in the graph <code>ComputeNext</code> is invoked unconditionally. In effect, this implements the behavior of an if-then-else conditional execution in the workflow.</p>"},{"location":"conditional/#constraints","title":"Constraints","text":"<p>There are constraints that need to be observed when expressing conditional execution:</p> <ul> <li>Cycles are not allowed (the workflow graph must still be a DAG)</li> <li>A node in a graph cannot have incident edges of different types (conditional and unconditional). </li> </ul> <p>The example below shows an invalid use of conditionals where <code>ComputeNext</code> has incident edges of different types:</p> <p></p>"},{"location":"contact/","title":"Contact","text":""},{"location":"contact/#connect-with-other-users","title":"Connect with other users","text":"<p>Join the FaaSr users' group to ask questions and connect with other users</p>"},{"location":"contact/#report-bugs-feature-requests","title":"Report bugs, feature requests","text":"<p>Submit an issue on GitHub if you would like to report a bug or feature request</p>"},{"location":"contact/#contact-project-leads","title":"Contact project leads","text":"<p>Send an email to the project development leads if you have other inquiries:</p> <ul> <li>Dr. Renato Figueiredo, Oregon State University</li> <li>Dr. R. Quinn Thomas, Virginia Tech</li> <li>Dr. Cayelan C. Carey, Virginia Tech</li> </ul>"},{"location":"credentials/","title":"Creating cloud credentials","text":""},{"location":"credentials/#s3-data-server","title":"S3 data server","text":"<ul> <li>In general, the credentials you need from your S3 account are the <code>AccessKey</code> and the <code>SecretKey</code>. These are akin to user names and passwords.</li> <li>How you obtain these credentials will depend on your provider (e.g. AWS S3, MINIO, OSN see the S3 document for additional information)</li> <li>Paste the access key and the secret key as Repository secrets in your FaaSr-workflow as per the instructions in the workflow repo documentation</li> </ul>"},{"location":"credentials/#github-actions","title":"GitHub Actions","text":"<p>If you don't already have one, you need to generate a GitHub Personal Access Token (PAT) to configure FaaSr. Details on how to create a PAT are available here</p> <p>In summary:</p> <ul> <li>In the upper-right corner of any page, click your profile photo, then click Settings.</li> <li>In the left sidebar, click Developer settings.</li> <li>In the left sidebar, click Personal access tokens.</li> <li>Click Generate new token.</li> <li>In the \"Note\" field, give your token a descriptive name.</li> <li>In scopes, select \u201cworkflow\u201d and \u201cread:org\u201d (under admin:org)</li> <li>Copy the token; you may save it in your local computer's password manager as well</li> <li>Paste the token under the name <code>GH_PAT</code> as a Repository secret in your FaaSr-workflow as per the instructions in the workflow repo documentation</li> </ul>"},{"location":"credentials/#aws-lambda","title":"AWS Lambda","text":"<ul> <li>You need an access key, secret key, and ARN to use Lambda in FaaSr.</li> <li>You can access your access and secret keys, and your ARN from your Amazon AWS console.</li> <li>Paste the access key and the secret key under the names <code>AWS_AccessKey</code> and <code>AWS_SecretKey</code>, respectively, as Repository secrets in your FaaSr-workflow as per the instructions in the workflow repo documentation</li> <li>Paste your ARN as <code>AWS_ARN</code> as Repository secrets in your FaaSr-workflow</li> </ul>"},{"location":"credentials/#openwhisk","title":"OpenWhisk","text":"<ul> <li>You need an API key from your provider to configure for use in FaaSr</li> <li>How you obtain this will depend on your cloud provider.</li> <li>Paste the API key under the name <code>OW_APIkey</code> as a Repository secret in your FaaSr-workflow as per the instructions in the workflow repo documentation</li> </ul>"},{"location":"credentials/#google-cloud-platform","title":"Google Cloud Platform","text":"<ul> <li>You need a private secret key to use Google Cloud Platform (GCP) with FaaSr</li> <li>You can access your key from the Google Cloud console</li> <li>TBD</li> </ul>"},{"location":"credentials/#slurm","title":"Slurm","text":"<p>TBD</p>"},{"location":"defaults/","title":"Default values","text":"<p>The following values are set by default in the Web UI and take effect unless overridden by the user:</p>"},{"location":"defaults/#compute-server-names","title":"Compute server names","text":"<ul> <li><code>GH</code>: GitHub Actions</li> <li><code>AWS</code>: AWS Lambda</li> <li><code>GCP</code>: Google Cloud</li> <li><code>OW</code>: OpenWhisk</li> <li><code>SLURM</code>: Slurm</li> </ul>"},{"location":"defaults/#compute-server-configurations","title":"Compute server configurations","text":""},{"location":"defaults/#githubactions","title":"GitHubActions","text":"<ul> <li><code>FaaSr-workflow</code>: workflow repository name</li> <li><code>main</code>: workflow repository branch</li> </ul>"},{"location":"defaults/#aws-lambda","title":"AWS Lambda","text":"<ul> <li><code>us-east-1</code>: region</li> </ul>"},{"location":"defaults/#openwhisk","title":"OpenWhisk","text":"<ul> <li><code>False</code>: allow self-signed certificates</li> </ul>"},{"location":"defaults/#data-store-configurations","title":"Data store configurations","text":"<ul> <li>'S3': default data store name</li> <li><code>True</code>: by default, an S3 store is set to writable</li> </ul>"},{"location":"defaults/#secrets","title":"Secrets","text":"<ul> <li><code>GH_PAT</code>: GitHub personal access token</li> <li><code>AWS_AccessKey</code>: AWS Lambda access key</li> <li><code>AWS_SecretKey</code>: AWS Lambda secret key</li> <li><code>GCP_SecretKey</code>: GCP secret key</li> <li><code>OW_APIkey</code>: OpenWhisk API key</li> <li><code>SLURM_Token</code>: SLURM JWT token</li> </ul>"},{"location":"defaults/#log-folder-in-s3","title":"Log folder (in S3)","text":"<ul> <li><code>FaaSrLog</code>: top-level log folder name; see log documentation for full path information</li> </ul>"},{"location":"defaults/#containers","title":"Containers","text":"<p>Note: the latest stable release is tagged as <code>:latest</code> in all containers; specific release versions are tagged with the version number, e.g. <code>:2.0.0</code> for version 2.0.0</p>"},{"location":"defaults/#python","title":"Python","text":"<ul> <li><code>ghcr.io/faasr/github-actions-python:latest</code>: GitHub Actions (in GHRC)</li> <li><code>145342739029.dkr.ecr.us-east-1.amazonaws.com/aws-lambda-python:latest</code>: AWS Lambda east-1 (in ECR)</li> <li><code>faasr/gcp-python:latest</code>: GCP (in DockerHub)</li> <li><code>faasr/openwhisk-python:latest</code>: OpenWhisk (in DockerHub)</li> <li><code>faasr/slurm-python:latest</code>: Slurm (in DockerHub)</li> </ul>"},{"location":"defaults/#r","title":"R","text":"<ul> <li><code>ghcr.io/faasr/github-actions-r:latest</code>: GitHub Actions (in GHRC)</li> <li><code>145342739029.dkr.ecr.us-east-1.amazonaws.com/aws-lambda-r:latest</code>: AWS Lambda east-1 (in ECR)</li> <li><code>faasr/gcp-r:latest</code>: GCP (in DockerHub)</li> <li><code>faasr/openwhisk-r:latest</code>: OpenWhisk (in DockerHub)</li> <li><code>faasr/slurm-r:latest</code>: Slurm (in DockerHub)</li> </ul>"},{"location":"dependences/","title":"Using package dependences","text":""},{"location":"dependences/#overview","title":"Overview","text":"<p>If a function has package dependences, it is possible to declare them when you create your workflow using the FaaSr Workflow Builder Web UI.  Dependences are downloaded and installed dynamically before your function is executed</p>"},{"location":"dependences/#r-dependences","title":"R dependences","text":"<p>When you create/edit an Action in the Web UI, scrolling the left pane to the bottom there are entries for:</p> <ul> <li>GitHub Packages for the Function: this can be used to install a package hosted in a GitHub repository. Provide the name of the GitHub repository to install from, for example</li> <li>CRAN Packages for the Function: this can be used to install a package hosted in CRAN. Provide the name of the package in the text box, for example ridigbio</li> </ul> <p>Import the dependence in your function, e.g. library('ridigbio')</p>"},{"location":"dependences/#python-dependences","title":"Python dependences","text":""},{"location":"docker_build/","title":"Process for building Docker containers","text":""},{"location":"docker_build/#faasr-docker-repository","title":"FaaSr-Docker repository","text":"<p>The FaaSr/FaaSr-Docker repository contains the Dockerfiles and entry points needed to build containers for the different supported platforms. If you want to build your own custom containers, you can fork this repository and configure it to build containers with your own environment and publish them in your own container registry account(s)</p>"},{"location":"docker_build/#secrets-needed","title":"Secrets needed","text":"<p>When you fork this repository, you need to set up the following GitHub Actions repository secrets:</p> <ul> <li><code>DOCKERHUB_USERNAME</code>: user name of account to publish images to DockerHub</li> <li><code>DOCKERHUB_TOKEN</code>: token to publish images to DockerHub</li> <li><code>AWS_ACCESS_KEY_ID</code>: access key to publish images to Amazon ECR</li> <li><code>AWS_SECRET_ACCESS_KEY</code>: secret key to publish images to Amazon ECR</li> </ul>"},{"location":"docker_build/#building-base-containers","title":"Building base containers","text":"<p>Before building any platform-specific containers (e.g. for GH, AWS, GCP), you need to build the base container. The platform-specific containers all build from this base container.</p>"},{"location":"docker_build/#building-python-base-container","title":"Building Python base container","text":"<ul> <li>In <code>Actions</code>, select <code>py-base-&gt;DockerHub</code></li> <li>Click on <code>Run workflow</code></li> <li>In <code>username:tag</code> to build from, typically you will leave the default (e.g. <code>python:3.13</code>)</li> <li>In <code>name to be used for this base FaaSr image</code> you also typically leave the default, <code>base-python</code></li> <li>In <code>FaaSr version tag</code>, enter the tag of the <code>FaaSr-Backend</code> repository to build from. Typically, this will be the current release of FaaSr, e.g. <code>2.1.0</code></li> <li>Run the workflow; once completed, it will publish the image to your DockerHub account, e.g. <code>yourname/base-python:2.1.0</code></li> </ul>"},{"location":"docker_build/#building-rrocker-base-container","title":"Building R/Rocker base container","text":"<ul> <li>In <code>Actions</code>, select <code>rocker-base-&gt;DockerHub</code></li> <li>Click on <code>Run workflow</code></li> <li>In <code>username:tag</code> to build from, typically you will leave the default (e.g. <code>rocker/tidyverse:4.4</code>)</li> <li>In <code>name to be used for this base FaaSr image</code> you also typically leave the default, <code>base-r</code></li> <li>In <code>FaaSr version tag</code>, enter the tag of the <code>FaaSr-Backend</code> repository to build from. Typically, this will be the current release of FaaSr, e.g. <code>2.1.0</code></li> <li>Run the workflow; once completed, it will publish the image to your DockerHub account, e.g. <code>yourname/base-r:2.1.0</code></li> </ul>"},{"location":"docker_build/#building-platform-specific-containers","title":"Building platform-specific containers","text":"<p>Once you have successfully built the base containers for Python and/or R, you will be able to build the following platform-specific containers</p>"},{"location":"docker_build/#github-actions","title":"GitHub Actions","text":"<ul> <li>Select Action <code>github-actions-&gt;GHCR</code></li> <li>Click <code>Run workflow</code></li> <li>In <code>user/name:tag of the base FaaSr image</code>, enter the DockerHub name of one of the image you created above, e.g. <code>yourname/base-python:2.1.0</code> or <code>yourname/base-r:2.1.0</code></li> <li>In <code>name of the FaaS-specific image to build</code>, enter the name of the image. This does not include your account name or tag - e.g. for python the default names we use are: <code>github-actions-python</code> and for r <code>github-actions-r</code></li> <li>In <code>FaaSr-py version</code> enter the same release number of the FaaSr-Backend used when building the base image, e.g. <code>2.1.0</code></li> <li>In <code>GitHub repo to install FaaSr-py from</code> enter the repo to build the Backend from. Typically this is the main <code>faasr/FaaSr-Backend</code> repo, but you can build from your own repo as well (e.g. if have forked FaaSr-Backend to develop and test a new feature)</li> <li>In <code>GitHub Container Repository (GHCR) to push image to</code> enter the name of your github account to publish the image to</li> <li>Run the workflow; once completed, it will publish the image to your GHCR account, e.g. <code>ghcr.io/yourusername/github-actions-python:2.1.0</code></li> </ul>"},{"location":"docker_build/#openwhisk","title":"OpenWhisk","text":"<ul> <li>Select Action <code>openwhisk-&gt;DockerHub</code></li> <li>Click <code>Run workflow</code></li> <li>In <code>user/name:tag of the base FaaSr image</code>, enter the DockerHub name of one of the image you created above, e.g. <code>yourname/base-python:2.1.0</code> or <code>yourname/base-r:2.1.0</code></li> <li>In <code>name of the FaaS-specific image to build</code>, enter the name of the image. This does not include your account name or tag - e.g. for python the default names we use are: <code>openwhisk-python</code> and for r <code>openwhisk-r</code></li> <li>In <code>FaaSr-py version</code> enter the same release number of the FaaSr-Backend used when building the base image, e.g. <code>2.1.0</code></li> <li>In <code>GitHub repo to install FaaSr-py from</code> enter the repo to build the Backend from. Typically this is the main <code>faasr/FaaSr-Backend</code> repo, but you can build from your own repo as well (e.g. if have forked FaaSr-Backend to develop and test a new feature)</li> <li>Run the workflow; once completed, it will publish the image to your DockerHub account, e.g. <code>yourusername/github-actions-python:2.1.0</code></li> </ul> <p>There is also an arm64 build process available for OpenWhisk - simply follow similar steps to build the base and OpenWhisk images as above, but use <code>py-base-arm-&gt;DockerHub</code> and <code>openwhisk-arm-&gt;DockerHub</code> instead</p>"},{"location":"docker_build/#aws-lambda","title":"AWS Lambda","text":"<ul> <li>Select Action <code>aws-lambda-&gt;ECR</code></li> <li>Click <code>Run workflow</code></li> <li>In <code>user/name:tag of the base FaaSr image</code>, enter the DockerHub name of one of the image you created above, e.g. <code>yourname/base-python:2.1.0</code> or <code>yourname/base-r:2.1.0</code></li> <li>In <code>name of the FaaS-specific image to build</code>, enter the name of the image. This does not include your account name or tag - e.g. for python the default names we use are: <code>aws-lambda-python</code> and for r <code>aws-lambda-r</code></li> <li>In <code>FaaSr-py version</code> enter the same release number of the FaaSr-Backend used when building the base image, e.g. <code>2.1.0</code></li> <li>In <code>GitHub repo to install FaaSr-py from</code> enter the repo to build the Backend from. Typically this is the main <code>faasr/FaaSr-Backend</code> repo, but you can build from your own repo as well (e.g. if have forked FaaSr-Backend to develop and test a new feature)</li> <li>In <code>AWS ECR region to push image to</code> enter the region of ECR to publish to</li> <li>Run the workflow; once completed, it will publish the image to your DockerHub account, e.g. <code>yourECR/aws-lambda-python:2.1.0</code></li> </ul>"},{"location":"docker_build/#google-cloud","title":"Google Cloud","text":"<ul> <li>Select Action <code>gcp-&gt;DockerHub</code></li> <li>Click <code>Run workflow</code></li> <li>In <code>user/name:tag of the base FaaSr image</code>, enter the DockerHub name of one of the image you created above, e.g. <code>yourname/base-python:2.1.0</code> or <code>yourname/base-r:2.1.0</code></li> <li>In <code>name of the FaaS-specific image to build</code>, enter the name of the image. This does not include your account name or tag - e.g. for python the default names we use are: <code>gcp-python</code> and for r <code>gcp-r</code></li> <li>In <code>FaaSr-py version</code> enter the same release number of the FaaSr-Backend used when building the base image, e.g. <code>2.1.0</code></li> <li>In <code>GitHub repo to install FaaSr-py from</code> enter the repo to build the Backend from. Typically this is the main <code>faasr/FaaSr-Backend</code> repo, but you can build from your own repo as well (e.g. if have forked FaaSr-Backend to develop and test a new feature)</li> <li>Run the workflow; once completed, it will publish the image to your DockerHub account, e.g. <code>yourusername/gcp-python:2.1.0</code></li> </ul>"},{"location":"docker_build/#slurm","title":"Slurm","text":"<ul> <li>Select Action <code>slurm-&gt;DockerHub</code></li> <li>Click <code>Run workflow</code></li> <li>In <code>user/name:tag of the base FaaSr image</code>, enter the DockerHub name of one of the image you created above, e.g. <code>yourname/base-python:2.1.0</code> or <code>yourname/base-r:2.1.0</code></li> <li>In <code>name of the FaaS-specific image to build</code>, enter the name of the image. This does not include your account name or tag - e.g. for python the default names we use are: <code>slurm-python</code> and for r <code>slurm-r</code></li> <li>In <code>FaaSr-py version</code> enter the same release number of the FaaSr-Backend used when building the base image, e.g. <code>2.1.0</code></li> <li>In <code>GitHub repo to install FaaSr-py from</code> enter the repo to build the Backend from. Typically this is the main <code>faasr/FaaSr-Backend</code> repo, but you can build from your own repo as well (e.g. if have forked FaaSr-Backend to develop and test a new feature)</li> <li>Run the workflow; once completed, it will publish the image to your DockerHub account, e.g. <code>yourusername/slurm-python:2.1.0</code></li> </ul>"},{"location":"docker_build/#customizing-dockerfiles","title":"Customizing Dockerfiles","text":"<p>If you need to customize your Dockerfiles with particular dependences, these can be found in the <code>faas_specific</code> folder. Edit as needed to add your dependences; you must not change the <code>WORKDIR</code>, <code>ARG</code>, <code>RUN</code>, <code>COPY</code>, <code>FROM</code> and <code>CMD</code> statements - you can only add dependences.</p>"},{"location":"functionexamples/","title":"Function examples","text":"<p>The FaaSr-Functions repository has a collection of examples of FaaSr workflows and R/Python functions that you can use as a starting template to develop your own workflows.</p> <p>Each folder in this repository includes:</p> <ul> <li>A JSON workflow configuration file which can be imported directly into the FaaSr Workflow Builder Web UI</li> <li>One or more source code functions</li> <li>A README document</li> </ul>"},{"location":"functions/","title":"Creating functions","text":""},{"location":"functions/#overview","title":"Overview","text":"<p>Creating a function for use in FaaSr entails the following steps:</p> <ul> <li>Select a GitHub repository to store your function code; we'll use MyGitHubAccount and MyFunctionRepo as a GitHub account name and repository name, respectively, in examples</li> <li>Develop the code for your function. A best practice is to have one file per function; we will use <code>compute_sum.R</code> and <code>compute_sum.py</code> as examples</li> <li>Add FaaSr API calls where appropriate, e.g. <code>faasr_get_file()</code> to get an input file from an S3 data server, <code>faasr_put_file()</code> to put an output file to an S3 data server, <code>faasr_log()</code> to write a message to the log. Refer to the FaaSr R APIs and FaaSr Python APIs documents for a complete list</li> <li>Add the function to a workflow. This is done using the FaaSr Workflow Builder Web UI by clicking on an Action in the workflow DAG</li> </ul>"},{"location":"functions/#example","title":"Example","text":"<p>Let's say you develop a function <code>compute_sum.R</code> (e.g. the one used in the FaaSr tutorial) as follows:</p> <pre><code>compute_sum &lt;- function(folder, input1, input2, output) {\n\n  # FaaSr API calls to get inputs from S3 (two CSV files)\n  faasr_get_file(remote_folder=folder, remote_file=input1, local_file=\"input1.csv\")\n  faasr_get_file(remote_folder=folder, remote_file=input2, local_file=\"input2.csv\")\n\n  # Function's main implementation (compute a sum and write the output)\n  frame_input1 &lt;- read.table(\"input1.csv\", sep=\",\", header=T)\n  frame_input2 &lt;- read.table(\"input2.csv\", sep=\",\", header=T)\n  frame_output &lt;- frame_input1 + frame_input2\n  write.table(frame_output, file=\"output.csv\", sep=\",\", row.names=F, col.names=T)\n\n  # FaaSr API call to put the output file in the S3 bucket\n  faasr_put_file(local_file=\"output.csv\", remote_folder=folder, remote_file=output)\n\n  # Log a message\n  log_msg &lt;- paste0('Function compute_sum finished; output written to ', folder, '/', output, ' in default S3 bucket')\n  faasr_log(log_msg)\n}   \n</code></pre> <p>Say you commit <code>compute_sum.R</code> to repository <code>MyGitHubAccount/MyFunctionRepo</code>. </p> <p>To use this function in a workflow, in the FaaSr Workflow Builder Web UI proceed as follows:</p> <p></p> <ul> <li>Create an Action (e.g. compute_sum)</li> <li>Select it to edit using the left pane</li> <li>Under Function Name, enter <code>compute_sum</code>; this is the name of the function declared in the code above</li> <li>Under Compute Server, select your compute server from the drop-down menu (e.g. GH for GitHub Actions)</li> <li>Under Arguments, enter names and values of the arguments matching those used by the function: <code>folder</code>, <code>input1</code>, <code>input2</code>, <code>output</code></li> <li>Under Function's Git Repo/Path, enter <code>MyGitHubAccount/MyFunctionRepo</code></li> </ul>"},{"location":"functions/#important-notes","title":"Important notes","text":"<ul> <li>If you provide a GitHub repo (e.g. <code>MyGitHubAccount/MyFunctionRepo</code>) Function's Git Repo/Path, FaaSr will clone and source all source code files from it; e.g. if your repository has files <code>compute_sum.R</code>, <code>compute_mult.R</code>, etc, each will be sourced</li> <li>You can, alternatively, provide a path to a file in a GitHub repo, e.g. <code>MyGitHubAccount/MyFunctionRepo/compute_sum.R</code>; this will only fetch and source one function</li> </ul>"},{"location":"invocationid/","title":"Invocation IDs","text":"<p>Each FaaSr workflow can be tagged with a unique ID. This is useful for two key reasons:</p> <ul> <li>It determines where logs are stored</li> <li>It allows you to derive unique names from your FaaSr functions, e.g. to create unique file names</li> </ul> <p>FaaSr allows you to specify a method for your unique invocation ID in one of three choices:</p> <ul> <li>Randomly-generated UUID (default)</li> <li>Timestamp-derived ID</li> <li>User-provided ID</li> </ul> <p>The invocation ID can be retrieved in your user functions using the R API or Python API, respectively.</p>"},{"location":"invocationid/#default-randomly-generated-uuid","title":"Default: randomly-generated UUID","text":"<p>This is the default method in the WebUI, and the behavior is as follows:</p> <ul> <li>A UUID is generated randomly in the entry action of your workflow, e.g. <code>550e8400-e29b-41d4-a716-446655440000</code></li> <li>The UUID generated in the entry action is automatically propagated to all actions in your workflow</li> </ul>"},{"location":"invocationid/#timestamp-derived-id","title":"Timestamp-derived ID","text":"<p>This method can be selected in the WebUI under Workflow Settings in the drop-down menu for InvocationID. If selected, you need to also determine the Timestamp format to be used to derive the Invocation ID. The behavior is as follows:</p> <ul> <li>A timestamp string is generated in the entry action of your workflow based on the UTC local time of the invocation of the action</li> <li>The format of the timestamp is defined as a string that follows the strftime format from the Python datetime class specifies which fields of the data/time are used. This depends on your particular use case - e.g. for a timestamp that is unique daily you can use <code>%Y%m%d</code>; unique hourly: <code>%Y%m%d%H</code>; unique per minute: <code>%Y%m%d%H%M</code></li> <li>Example: Suppose the entry action is dispatched on October 21, 2025 at 11:03pm (or 23:03). A timestamp format for daily, i.e. <code>%Y%m%d</code> generates an invocation ID that looks like <code>20251021</code>. A timestamp format for hourly, i.e. <code>%Y%m%d%H</code>, generates an invocation ID <code>2025102123</code>. A timestamp format for per-minute, i.e. <code>%Y_%m_%d_%H_%M</code>, generates an invocation ID <code>2025_10_21_23_03</code></li> </ul>"},{"location":"invocationid/#user-provided-id","title":"User-provided ID","text":"<p>Finally, in the user-provided ID, the invocation ID is provided by the user in the JSON configuration.</p>"},{"location":"invoke_workflow/","title":"Invoking workflows","text":"<p>After successfully registering a workflow, the workflow is ready to be invoked. This is implemented by the GitHub Action <code>FAASR INVOKE</code>, also available in your FaaSr-workflow repo once you fork it. In short, <code>FAASR INVOKE</code> \"kick-starts\" your workflow by invoking its entry point action; subsequent actions are then executed according to the order implied by your workflow DAG as per the programming model.</p>"},{"location":"invoke_workflow/#requirements","title":"Requirements","text":"<p>To invoke a workflow, you need:</p> <ul> <li>Your forked FaaSr-workflow repo</li> <li>A JSON workflow configuration file, downloaded from the workflow Web UI and uploaded to your FaaSr-workflow repo, successfully registered with <code>FAASR REGISTER</code></li> <li>Proper cloud credentials for any cloud compute servers and data stores you use, stored as GitHub Secrets. These secrets must also be copied and available in cloud platforms that have native secret stores, AWS and GCP</li> </ul>"},{"location":"invoke_workflow/#running-faasr-invoke","title":"Running FAASR INVOKE","text":"<ul> <li>The workflow invocation action <code>FAASR INVOKE</code> is available under the <code>Actions</code> tab in your FaaSr-workflow repo (it is named such that it appears near the top of the action list).</li> <li>To execute, select this action from the list, then click on the drop-down <code>Run workflow</code> menu to the right, and enter the name of the JSON workflow configuration file.</li> <li>If invocation is successful, each action in your workflow results in the execution of a a corresponding action in the cloud provider you use: these could be GitHub actions (for GH compute servers), AWS Lambdas (for AWS), Google Cloud Run jobs (for GCP), OpenWhisk actions (for OW), or Slurm jobs.</li> <li>The naming scheme used for the actions invoked throughout the workflow by <code>FAASR INVOKE</code> is as follows: <code>WorkflowName-ActionName</code> where <code>WorkflowName</code> is the workflow name defined in your configuration file (under <code>Workflow settings</code> in the workflow Web UI), and <code>ActionName</code> is the name of the action (each node of the graph in the workflow Web UI).</li> <li>For example, for the FaaSr tutorial, the workflow name is <code>tutorial</code> and there are two actions in the graph (<code>start</code> and <code>sum</code>); correspondingly, two GitHub Actions are executed, in sequence, namely: <code>tutorial-start</code> followed by <code>tutorial-sum</code>.</li> <li>If any errors occur during invocation, a red <code>X</code> icon will appear after the action completes. Click on the action name and then on the <code>deploy</code> box to review logs for errors.</li> <li>Errors that may happen during the workflow execution will appear with the same red <code>X</code> icon, if you use GH as your cloud compute platform. If you invoke workflows in other platforms (AWS, GCP, OW, SLURM), error logs will be available on that cloud platform rather than on GitHub Actions.</li> </ul>"},{"location":"logs/","title":"Logs","text":""},{"location":"logs/#faasr-logs","title":"FaaSr logs","text":"<p>Logs produced by FaaSr functions using the <code>faasr_log()</code> API are stored in an S3 server. This document overviews how to select an S3 server to store these logs, and the folder naming structure to help you locate a particular log</p>"},{"location":"logs/#selecting-an-s3-bucket-for-logs","title":"Selecting an S3 bucket for logs","text":"<ul> <li>In the typical scenario, a single S3 server/bucket can be used for both workflow files and logs, but you may also use separate buckets</li> <li>The S3 server used for logs is configured as the Default server to store logs under Workflow settings in the FaaSr Workflow Builder Web UI </li> </ul>"},{"location":"logs/#log-folder-naming-convention","title":"Log folder naming convention","text":"<ul> <li>Using the FaaSr Workflow Builder Web UI you must name your workflow uniquely; we'll use <code>WorkflowName</code> in this example</li> <li>Using the FaaSr Workflow Builder Web UI you can also provide a name for the top-level log folder; if you do not provide one, by default the name is is set to <code>FaaSrLog</code></li> <li>Then, the path where you can find logs for a particular Action function execution is:</li> </ul> <p><code>FaaSrLog/WorkflowName/InvocationTimestamp/InvocationID/ActionName.txt</code></p> <p>Where: - <code>ActionName</code> is the name of the action (i.e. the name of a node in your DAG) - <code>InvocationID</code> is a specific invocation of your Action. Refer to the invocationID documentation for the different types of invocation IDs supported (UUID, user-provided, and timestamp-derived) - <code>InvocationTimestamp</code> is a string-formatted invocation timestamp recorded at the time your workflow's entry action starts execution. It is recorded as an ISO 8601-like timestamp with filename-safe separators</p>"},{"location":"logs/#examples","title":"Examples","text":"<ul> <li>Suppose you run the FaaSr tutorial as an example</li> <li>In this workflow configuration, the log folder is the default <code>FaaSrLog</code> and the workflow name is <code>FaaSrTutorial</code></li> <li>Suppose you also leave the <code>InvocationID</code> as default, which generates a unique UUID automatically at the start entry point. In this example, let's assume the UUID generated is: <code>f81d4fae-7dec-11d0-a765-00a0c91e6bf6</code></li> <li>Suppose when you invoke the workflow, the current time seen by the start action was timestamped as <code>2025-08-29T23-17-01</code></li> </ul> <p>You will then be able to retrieve logs for the compute_sum action from the (unique) path: </p> <p><code>FaaSrLog/FaaSrTutorial/2025-08-29T23-17-01/f81d4fae-7dec-11d0-a765-00a0c91e6bf6/compute_sum.txt</code></p> <p>If you invoke the workflow again, a new timestamp and UUID will be generated</p>"},{"location":"logs/#cloud-logs","title":"Cloud logs","text":"<ul> <li>In addition to FaaSr-specific logs (i.e. where you use the <code>faasr_log()</code> API), you will find provider-specific action logs in your cloud provider of choice</li> <li>Please refer to their documentation:</li> <li>GitHub Action logs</li> <li>AWS Lambda logs</li> <li>Google cloud logs</li> </ul>"},{"location":"prog_model/","title":"FaaSr workflow model","text":"<p>The FaaSr model for programming, configuring, and deploying workflows is as follows:</p> <ul> <li>FaaSr Workflow DAG: describes the order in which your functions execute. Valid workflows must be Directed Acyclic Graphs (DAG) (i.e. no loops) with a single invocation entry, and are described as a JSON file</li> <li>Action: each node in the DAG is an Action that represents the execution of a user function in a container deployed at runtime in a serverless cloud provider (e.g. a GitHub Action, or AWS Lambda)</li> <li>Function: once deployed, each Action executes a user-defined Function in the contaier. A Function is written in a high level language and published as a .R or .py file in a GitHub Function repo. A Function takes input arguments and returns either true (successful excecution) or false (unsuccessful execution)</li> <li>Input/output: a serverless Action is stateless: the local memory and storage in a container is discarded when the Action completes. Therefore, all data that must persist across invocations must be stored as files in one or more S3 cloud data store servers. FaaSr provides an Application Programming Interface (API) to store/retrieve files to/from S3 for this.</li> </ul>"},{"location":"prog_model/#workflow-dag-example","title":"Workflow DAG example","text":"<p>To illustrate how a FaaSr workflow DAG works, consider the example below:</p> <p></p> <ul> <li>This DAG has seven Actions, where the invocation entry is a node named start</li> <li>Action start has two successor actions: computeA and computeB; this means that when start finishes executing, both computeA and computeB are invoked, concurrently</li> <li>Action computeA invokes ten instances of action concurrent; each of these actions is provided with a unique Rank (in this example, a number from 1 to 10)</li> <li>Action computeB invokes either conditionalT or conditionalF, depending on whether computeB returns True or False, respectively</li> <li>Finally, action end only executes after all its predecessors in the DAG finish their execution - i.e. after all 10 instances of concurrent and either conditionalT or conditionalF finish</li> <li>In this DAG, the concurrent actions execute as AWS Lambdas, while all other actions execute as GitHub Actions</li> </ul>"},{"location":"prog_model/#actions-functions-and-inputoutput-files","title":"Actions, functions, and input/output files","text":"<p>The example workflow DAG above names which actions should execute, and in which order. Each action is essentially a container (e.g. a Docker instance) that is deployed on a serverless cloud provider - which means these actions are invoked, execute for some time, then are terminated. Let's dive deeper now into: how to define which Function each action executes? How to save and retrieve inputs and outputs?</p> <p></p> <ul> <li>Action computeA runs a function written in R, while concurrent runs a function written in Python. These stored in (one or more) GitHub function repo(s) and are fetched automatically by the action when it is invoked</li> <li>Actions can read input files and write output files to (one or more) S3 data store(s). the FaaSr API provides functions in R and Python to put files to/get files from data server(s). The FaaSr API simplifies the programming of functions to use data from S3, without exposing you (the programmer or user) to details on how to access S3</li> <li>In general, the bulk of the data in a workflow consists of files; function arguments are used for configuration (e.g. of parameters, file and folder names)</li> <li>The typical file access pattern is as follows: 1) an action starts; 2) the action gets input file(s), copying them from persistent S3 storage into its local temporary storage; 3) the action produces output file(s) in its local non-persistent storage; 4) the action puts its output file(s) to persistent S3 storage; and 5) the action ends </li> </ul>"},{"location":"prog_model/#creating-editing-registering-and-invoking-workflows","title":"Creating, editing, registering and invoking workflows","text":"<p>We've seen what the workflow DAG represents (actions and their orders) and we also have seen that workflow DAGs themselves are stored in files in the JSON format. Putting it all together, we need ways to: 1) create and edit workflow JSON files using the FaaSr workflow builder Web graphical user interface; 2) store these workflow JSON configuration files in a FaaSr-workflow repository on GitHub; and 3) register and invoke these workflows in your cloud provider(s) of choice. Consider the example below that illustrates how this process typically works:</p> <p></p> <ul> <li>The user first needs to fork the FaaSr-workflow repo from the FaaSr organization. This will be your main environment for managing all your workflows (you can also have multiple workflow repos, though typically a single repo will suffice)</li> <li>Let's assume the user already has one or more workflows in this repo (e.g. the tutorial workflow). Then the user can use the FaaSr workflow builder to 1) import the workflow JSON file from their FaaSr-workflow repo; 2) edit with the Web UI; 3) download the JSON file to their computer; 4) commit the JSON file to their workflow repo; 5) register the workflow with their cloud provider(s); and 6) invoke the workflow so it executes in the cloud</li> <li>Once a workflow is registered, it can be invoked multiple times, either as manual, user-initiated one-shot invocations, or as an automatic, unattended periodic timer invocation</li> <li>The registration and invocation of workflows are themselves implemented as GitHub Actions inherited from the forked FaaSr-workflow repo</li> <li>The FaaSr-workflow repo also stores Secrets which are the credentials needed to access your cloud provider(s) of choice</li> </ul>"},{"location":"prog_model/#container-images-and-package-dependences","title":"Container images and package dependences","text":"<ul> <li>FaaSr provides a set of base images for containers with runtime environments for both Python and R, and deployable on the supported cloud providers. These images are typically sufficient for most use cases.</li> <li>If a function needs additional packages (e.g. from CRAN, PyPI), those can be declared in the workflow JSON configuration and fetched automatically.</li> <li>If a function needs a custom container image, the user can create their own custom container image(s), copy them to the appropriate container registry (e.g. DockerHub, GHCR, ECR), and also declare them in the workflow JSON configuration</li> </ul>"},{"location":"pull_requests/","title":"Process for issuing Pull Requests (PRs)","text":""},{"location":"py_api/","title":"Py APIs","text":""},{"location":"py_api/#faasr_get_file","title":"faasr_get_file","text":"<p>Usage: <code>faasr_get_file(server_name, remote_folder, remote_file, local_folder, local_file)</code></p> <p>This function gets (i.e. downloads) a file from an S3 bucket to be used by the FaaSr function.</p> <p><code>server_name</code> is a string with name of the S3 bucket to use; it must match a name declared in the workflow configuration JSON file. This is an optional argument; if not provided, the default S3 server specified as <code>DefaultDataStore</code> in the workflow configuration JSON file is used.</p> <p><code>remote_folder</code> is string with the name of the remote folder where the file is to be downloaded from. This is an optional argument that defaults to <code>\"\"</code></p> <p><code>remote_file</code> is a string with the name for the file to be downloaded from the S3 bucket. This is a required argument.</p> <p><code>local_folder</code> is a string with the name of the local folder where the file to be downloaded is stored. This is an optional argument that defaults to <code>\".\"</code></p> <p><code>local_file</code> is a string with the name for the file downloaded from the S3 bucket. This is a required argument.</p> <p>Examples:</p> <pre><code>faasr_get_file(remote_folder=\"myfolder\", remote_file=\"myinput1.csv\", local_file=\"input1.csv\")\nfaasr_get_file(server_name=\"My_Minio_Bucket\", remote_file=\"myinput2.csv\", local_file=\"input2.csv\")\n</code></pre>"},{"location":"py_api/#faasr_put_file","title":"faasr_put_file","text":"<p>Usage: <code>faasr_put_file(server_name, local_folder, local_file, remote_folder, remote_file)</code></p> <p>This function puts (i.e. uploads) a file from the local FaaSr function to an S3 bucket.</p> <p><code>server_name</code> is a string with name of the S3 bucket to use; it must match a name declared in the workflow configuration JSON file. This is an optional argument; if not provided, the default S3 server specified as <code>DefaultDataStore</code> in the workflow configuration JSON file is used.</p> <p><code>local_folder</code> is a string with the name of the local folder where the file to be uploaded is stored. This is an optional argument that defaults to <code>\".\"</code></p> <p><code>local_file</code> is a string with the name for the file to be uploaded to the S3 bucket. This is a required argument.</p> <p><code>remote_folder</code> is string with the name of the remote folder where the file is to be uploaded to. This is an optional argument that defaults to <code>\"\"</code></p> <p><code>remote_file</code> is a string with the name for the file to be uploaded to the S3 bucket. This is a required argument.</p> <p>Examples:</p> <pre><code>faasr_put_file(local_file=\"output.csv\", remote_folder=\"myfolder\", remote_file=\"myoutput.csv\")\nfaasr_get_file(server_name=\"My_Minio_Bucket\", local_file=\"output.csv\", remote_file=\"myoutput.csv\")\n</code></pre>"},{"location":"py_api/#faasr_get_folder_list","title":"faasr_get_folder_list","text":"<p>Usage: <code>folderlist = faasr_get_folder_list(server_name, faasr_prefix)</code></p> <p>This function returns a list with the contents of a folder in the S3 bucket. </p> <p><code>server_name</code> is a string with name of the S3 bucket to use; it must match a name declared in the workflow configuration JSON file. This is an optional argument; if not provided, the default S3 server specified as <code>DefaultDataStore</code> in the workflow configuration JSON file is used.</p> <p><code>faasr_prefix</code> is a string with the prefix of the folder in the S3 bucket. This is an optional argument that defaults to <code>\"\"</code></p> <p>Examples:</p> <pre><code>mylist1 = faasr_get_folder_list(server_name=\"My_Minio_Bucket\", faasr_prefix=\"myfolder\")\nmylist2 = faasr_get_folder_list(server_name=\"My_Minio_Bucket\", faasr_prefix=\"myfolder/mysubfolder\")\n</code></pre>"},{"location":"py_api/#faasr_delete_file","title":"faasr_delete_file","text":"<p>Usage: <code>faasr_delete_file(server_name, remote_folder, remote_file)</code></p> <p>This function deletes a file from the S3 bucket.</p> <p><code>server_name</code> is a string with name of the S3 bucket to use; it must match a name declared in the workflow configuration JSON file. This is an optional argument; if not provided, the default S3 server specified as <code>DefaultDataStore</code> in the workflow configuration JSON file is used.</p> <p><code>remote_folder</code> is string with the name of the remote folder where the file is to be deleted from. This is an optional argument that defaults to <code>\"\"</code></p> <p><code>remote_file</code> is a string with the name for the file to be deleted from the S3 bucket. This is a required argument.</p> <p>Examples:</p> <pre><code>faasr_delete_file(remote_folder=\"myfolder\", remote_file=\"myoutput.csv\")\nfaasr_delete_file(server_name=\"My_Minio_Bucket\", remote_file=\"myoutput.csv\")\n</code></pre>"},{"location":"py_api/#faasr_log","title":"faasr_log","text":"<p>Usage: <code>faasr_log(log_message)</code></p> <p>This function writes a log message to a file in the S3 bucket, to help with debugging. The default S3 server for logs is <code>DefaultDataStore</code> as specified in the workflow configuration JSON file. This default can be overridden with <code>LoggingDataStore</code> in the workflow configuration JSON file.</p> <p><code>log_message</code> is a string with the message to be logged.</p> <p>Example:</p> <pre><code>log_msg = f\"Function compute_sum finished; output written to {folder}/{output} in default S3 bucket\"\nfaasr_log(log_msg)\n</code></pre>"},{"location":"py_api/#faasr_rank","title":"faasr_rank","text":"<p>Usage: <code>faasr_rank()</code></p> <p>Only applicable for workflows with functions that are triggered for concurrent execution using rank. This returns this function's invocation rank as a dictionary with two keys:</p> <p><code>max_rank</code> - an integer that determines to the maximum number of invocations (e.g. N) <code>rank</code> - an integer that determines this function's invocation rank (e.g. a number ranging from 1 to N)</p> <p>Example:</p> <p>In a workflow, suppose you have declared that a function <code>start</code> invokes a successor function <code>compute</code> with rank N=10, where each instance of <code>compute</code> works to produce an output file <code>myoutput_i.csv</code>, where i is a number from 1 to 10.</p> <p>At runtime, <code>start</code> invokes 10 instances of <code>compute</code>; <code>faasr_rank()</code> determines the value of <code>i</code> for each instance of <code>compute</code>, which can be used to construct the file name:</p> <pre><code>rank_list = faasr_rank()\nmy_rank = rank_list[\"rank\"]\nmax_rank = rank_list[\"max_rank\"]\nlocal_file = f\"myinput_{my_rank}.csv\"\ndf.to_csv(local_file, index=False)\n</code></pre>"},{"location":"py_api/#faasr_invocation_id","title":"faasr_invocation_id","text":"<p>Usage: <code>faasr_invocation_id()</code></p> <p>Returns this action's invocation ID.</p> <p>Example:</p> <pre><code>invocation_id = faasr_invocation_id()\nprint(invocation_id)\n</code></pre>"},{"location":"r_api/","title":"R APIs","text":""},{"location":"r_api/#faasr_get_file","title":"faasr_get_file","text":"<p>Usage: <code>faasr_get_file(server_name, remote_folder, remote_file, local_folder, local_file)</code></p> <p>This function gets (i.e. downloads) a file from an S3 bucket to be used by the FaaSr function.</p> <p><code>server_name</code> is a string with name of the S3 bucket to use; it must match a name declared in the workflow configuration JSON file. This is an optional argument; if not provided, the default S3 server specified as <code>DefaultDataStore</code> in the workflow configuration JSON file is used.</p> <p><code>remote_folder</code> is string with the name of the remote folder where the file is to be downloaded from. This is an optional argument that defaults to <code>\"\"</code></p> <p><code>remote_file</code> is a string with the name for the file to be downloaded from the S3 bucket. This is a required argument.</p> <p><code>local_folder</code> is a string with the name of the local folder where the file to be downloaded is stored. This is an optional argument that defaults to <code>\".\"</code></p> <p><code>local_file</code> is a string with the name for the file downloaded from the S3 bucket. This is a required argument.</p> <p>Examples:</p> <pre><code>faasr_get_file(remote_folder=\"myfolder\", remote_file=\"myinput1.csv\", local_file=\"input1.csv\")\nfaasr_get_file(server_name=\"My_Minio_Bucket\", remote_file=\"myinput2.csv\", local_file=\"input2.csv\")\n</code></pre>"},{"location":"r_api/#faasr_put_file","title":"faasr_put_file","text":"<p>Usage: <code>faasr_put_file(server_name, local_folder, local_file, remote_folder, remote_file)</code></p> <p>This function puts (i.e. uploads) a file from the local FaaSr function to an S3 bucket.</p> <p><code>server_name</code> is a string with name of the S3 bucket to use; it must match a name declared in the workflow configuration JSON file. This is an optional argument; if not provided, the default S3 server specified as <code>DefaultDataStore</code> in the workflow configuration JSON file is used.</p> <p><code>local_folder</code> is a string with the name of the local folder where the file to be uploaded is stored. This is an optional argument that defaults to <code>\".\"</code></p> <p><code>local_file</code> is a string with the name for the file to be uploaded to the S3 bucket. This is a required argument.</p> <p><code>remote_folder</code> is string with the name of the remote folder where the file is to be uploaded to. This is an optional argument that defaults to <code>\"\"</code></p> <p><code>remote_file</code> is a string with the name for the file to be uploaded to the S3 bucket. This is a required argument.</p> <p>Examples:</p> <pre><code>faasr_put_file(local_file=\"output.csv\", remote_folder=\"myfolder\", remote_file=\"myoutput.csv\")\nfaasr_get_file(server_name=\"My_Minio_Bucket\", local_file=\"output.csv\", remote_file=\"myoutput.csv\")\n</code></pre>"},{"location":"r_api/#faasr_get_folder_list","title":"faasr_get_folder_list","text":"<p>Usage: <code>folderlist &lt;- faasr_get_folder_list(server_name, faasr_prefix)</code></p> <p>This function returns a list with the contents of a folder in the S3 bucket. </p> <p><code>server_name</code> is a string with name of the S3 bucket to use; it must match a name declared in the workflow configuration JSON file. This is an optional argument; if not provided, the default S3 server specified as <code>DefaultDataStore</code> in the workflow configuration JSON file is used.</p> <p><code>faasr_prefix</code> is a string with the prefix of the folder in the S3 bucket. This is an optional argument that defaults to <code>\"\"</code></p> <p>Examples:</p> <pre><code>mylist1 &lt;- faasr_get_folder_list(server_name=\"My_Minio_Bucket\", faasr_prefix=\"myfolder\")\nmylist2 &lt;- faasr_get_folder_list(server_name=\"My_Minio_Bucket\", faasr_prefix=\"myfolder/mysubfolder\")\n</code></pre>"},{"location":"r_api/#faasr_delete_file","title":"faasr_delete_file","text":"<p>Usage: <code>faasr_delete_file(server_name, remote_folder, remote_file)</code></p> <p>This function deletes a file from the S3 bucket.</p> <p><code>server_name</code> is a string with name of the S3 bucket to use; it must match a name declared in the workflow configuration JSON file. This is an optional argument; if not provided, the default S3 server specified as <code>DefaultDataStore</code> in the workflow configuration JSON file is used.</p> <p><code>remote_folder</code> is string with the name of the remote folder where the file is to be deleted from. This is an optional argument that defaults to <code>\"\"</code></p> <p><code>remote_file</code> is a string with the name for the file to be deleted from the S3 bucket. This is a required argument.</p> <p>Examples:</p> <pre><code>faasr_delete_file(remote_folder=\"myfolder\", remote_file=\"myoutput.csv\")\nfaasr_delete_file(server_name=\"My_Minio_Bucket\", remote_file=\"myoutput.csv\")\n</code></pre>"},{"location":"r_api/#faasr_arrow_s3_bucket","title":"faasr_arrow_s3_bucket","text":"<p>Usage: <code>faasr_arrow_s3_bucket(server_name, faasr_prefix)</code></p> <p>This function configures an S3 bucket to use with Apache Arrow.</p> <p><code>server_name</code> is a string with name of the S3 bucket to use; it must match a name declared in the workflow configuration JSON file. This is an optional argument; if not provided, the default S3 server specified as <code>DefaultDataStore</code> in the workflow configuration JSON file is used.</p> <p><code>faasr_prefix</code> is a string with the prefix of the folder in the S3 bucket. This is an optional argument that defaults to <code>\"\"</code></p> <p>It returns a list that is subsequently used with the Arrow package.</p> <p>Examples:</p> <pre><code>mys3 &lt;- faasr_arrow_s3_bucket()\nmyothers3 &lt;- faasr_arrow_s3_bucket(server_name=\"My_Minio_Bucket\", faasr_prefix=\"myfolder\")\nframe_input1 &lt;- arrow::read_csv_arrow(mys3$path(file.path(folder, input1)))\nframe_input2 &lt;- arrow::read_csv_arrow(mys3$path(file.path(folder, input2)))\narrow::write_csv_arrow(frame_output, mys3$path(file.path(folder, output)))\n</code></pre>"},{"location":"r_api/#faasr_log","title":"faasr_log","text":"<p>Usage: <code>faasr_log(log_message)</code></p> <p>This function writes a log message to a file in the S3 bucket, to help with debugging. The default S3 server for logs is <code>DefaultDataStore</code> as specified in the workflow configuration JSON file. This default can be overridden with <code>LoggingDataStore</code> in  the workflow configuration JSON file.</p> <p><code>log_message</code> is a string with the message to be logged.</p> <p>Example:</p> <pre><code>log_msg &lt;- paste0('Function compute_sum finished; output written to ', folder, '/', output, ' in default S3 bucket')\nfaasr_log(log_msg)\n</code></pre>"},{"location":"r_api/#faasr_rank","title":"faasr_rank","text":"<p>Usage: <code>faasr_rank()</code></p> <p>Only applicable for workflows with functions that are triggered for concurrent execution using rank. This returns this function's invocation rank as a list with two values:</p> <p><code>list$max_rank</code> - an integer that determines to the maximum number of invocations (e.g. N) <code>list$rank</code> - an integer that determines this function's invocation rank (e.g. a number ranging from 1 to N)</p> <p>Example:</p> <p>In a workflow, suppose you have declared that a function <code>start</code> invokes a successor function <code>compute</code> with rank N=10, where each instance of <code>compute</code> works to produce an output file <code>myoutput_i.csv</code>, where i is a number from 1 to 10.</p> <p>At runtime, <code>start</code> invokes 10 instances of <code>compute</code>; <code>faasr_rank()</code> determines the value of <code>i</code> for each instance of <code>compute</code>, which can be used to construct the file name:</p> <pre><code>rank_list &lt;- faasr_rank()\nmy_rank &lt;- rank_list$rank\nmax_rank &lt;- rank_list$max_rank\nlocal_file &lt;- paste0(\"myinput_\", my_rank, \".csv\")\nwrite.csv(my_data, local_file, row.names=FALSE)\n</code></pre>"},{"location":"r_api/#faasr_invocation_id","title":"faasr_invocation_id","text":"<p>Usage: <code>faasr_invocation_id()</code></p> <p>Returns this action's invocation ID.</p> <p>Example:</p> <pre><code>invocation_id &lt;- faasr_invocation_id()\n</code></pre>"},{"location":"rank/","title":"Concurrent Actions","text":"<p>FaaSr supports the invocation of multiple concurrent actions in the workflow. This is useful in workflows where the workflow developer is able to determine that multiple functions can safely execute in parallel - for example, when different functions operate on independent files/datasets There are two approaches possible to invoke actions to execute concurrently:</p> <ul> <li>Multiple distinct actions explicitly invoked with edges from the same predecessor in the workflow DAG, or</li> <li>A single action that is invoked N times, where N is a configurable integer number, greater than 1; each invocation of the action is assigned a unique rank (an integer from 1 to N) that is available to the user function via the <code>faasr_rank</code> API</li> </ul>"},{"location":"rank/#multiple-distinct-actions","title":"Multiple distinct actions","text":"<p>Consider the example below, where the action <code>DownloadData</code> invokes three concurrent actions that are distinct and explicitly added to the workflow graph: <code>ComputeA</code>, <code>ComputeB</code>, and <code>ComputeC</code>:</p> <p></p> <p>Each one of these actions can invoke a different user function concurrently. At the end, the action <code>Merge</code> executes the last user function, only after the three distinct actions <code>ComputeA</code>, <code>ComputeB</code>, and <code>ComputeC</code> have completed.</p>"},{"location":"rank/#ranked-actions","title":"Ranked actions","text":"<p>Ranked actions allows you to express concurrency with multiple, parameterized invocations of the same user function. Consider the workflow below, where the action <code>DownloadData</code> now invokes the same action <code>ComputeRanked</code> three times, concurrently:</p> <p></p> <p>Each concurrent <code>ComputeRanked</code> action invokes the same user function. Similarly to the previous example, at the end, the action <code>Merge</code> executes the last user function, only after all three <code>ComputeRanked</code> actions have completed.</p> <p>Inside the user function invoked by each <code>ComputeRanked</code> action, the <code>faasr_rank</code> API can be invoked to return a unique number (1, 2, or 3 in this example) that can be used to differentiate between invocations. As described in the Python API and R API documentation, <code>faasr_rank</code> allows the user function to determine the <code>maximum</code> rank (3 in this example) in addition to the <code>per-action</code> rank (1, 2, or 3). </p> <p>This information can be used, for example, to select different file names, folders, or data subsets that each action operates on. It is the responsibility of the programmer to properly use the rank information to partition datasets for safe concurrent execution, depending on their workflow and application logic.</p>"},{"location":"register_workflow/","title":"Registering workflows","text":"<p>The process of registering workflows is required to go from a workflow description (i.e. a FaaSr-compliant JSON configuration file) to a workflow that can be invoked. This is implemented by the GitHub Action <code>FAASR REGISTER</code> available in your FaaSr-workflow repo once you fork it.</p>"},{"location":"register_workflow/#requirements","title":"Requirements","text":"<p>To register a workflow, you need:</p> <ul> <li>Your forked FaaSr-workflow repo</li> <li>A JSON workflow configuration file, downloaded from the workflow Web UI and uploaded to your FaaSr-workflow repo</li> <li>Proper cloud credentials for any cloud compute servers and data stores you use, stored as GitHub Secrets</li> </ul>"},{"location":"register_workflow/#running-faasr-register","title":"Running FAASR REGISTER","text":"<ul> <li>The workflow registration action <code>FAASR REGISTER</code> is available under the <code>Actions</code> tab in your FaaSr-workflow repo (it is named such that it appears at the top of the action list). to execute, select this action from the list, then click on the drop-down <code>Run workflow</code> menu to the right, and enter the name of the JSON workflow configuration file.</li> <li>If your workflow uses custom containers, you are required to tick the checkbox <code>Allow custom containers</code>; please review the security best practices document.</li> <li>If any errors occur during registration, a red <code>X</code> icon will appear after the action completes. Click on the action name and then on the <code>deploy</code> box to review logs for errors.</li> <li>If registration is successful, each action in your workflow results in the creation (or update) of a corresponding action in the cloud provider you use: these could be GitHub actions (for GH compute servers), AWS Lambdas (for AWS), Google Cloud Run jobs (for GCP), OpenWhisk actions (for OW), or Slurm jobs.</li> <li>The naming scheme used for the actions created by <code>FAASR REGISTER</code> is as follows: <code>WorkflowName-ActionName</code> where <code>WorkflowName</code> is the workflow name defined in your configuration file (under <code>Workflow settings</code> in the workflow Web UI), and <code>ActionName</code> is the name of the action (each node of the graph in the workflow Web UI). For example, for the FaaSr tutorial, the workflow name is <code>tutorial</code> and there are two actions in the graph (<code>start</code> and <code>sum</code>); correspondingly, two GitHub Actions are created, named <code>tutorial-start</code> and <code>tutorial-sum</code>.</li> </ul>"},{"location":"s3/","title":"Obtaining an S3 bucket","text":"<p>If you do not already have an S3 bucket, several options are possible for you to get started: a temporary test bucket, free-tier and commercial S3 providers, and academic providers. </p> <p>The following is a non-exhaustive list of options - in general, any S3-compatible server can be used with FaaSr.</p>"},{"location":"s3/#minio-play-temporary-s3-test-bucket","title":"MinIO Play - temporary S3 test bucket","text":"<p>MinIO Play is a shared \"playground\" environment for Internet users to try out S3. When you follow the tutorial for the first time, you use this bucket: it allows easy access to try things out quickly. However, it's a temporary solution - it does not keep your data persistent or private.</p> <p>Its credentials that you use when setting up your workflow repo are common to all users, as follows: - Access key: <code>Q3AM3UQ867SPQQA43P2F</code> - Secret key: <code>zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG</code></p>"},{"location":"s3/#backblaze-free-tier-without-credit-card","title":"Backblaze - free-tier without credit card","text":"<p>Backblaze offers a free-tier account that you can setup quickly without a credit card:</p> <ul> <li>Follow their instructions to create a new account using your email address</li> <li>When prompted to select your region, any choice is acceptable. In this example we'll use US-east</li> <li>Once you sign-in successfully with your account, click on \"Create a Bucket\"</li> <li>Give your bucket a unique string name - this will be needed to configure FaaSr workflows with the workflow builder. We'll use myuniquebucket as an example here</li> <li>Leave the encryption and lock options as disabled (default)</li> <li>Once your bucket is created, you need also to add a new Application Key - this will be used as credentials in your workflow repo</li> <li>Click on \"Add Application Key\", and give the key a unique name (e.g. myFaaSrKey). Leave the default \"read and write\" setting</li> <li>Once the key is created, you need to save the KeyID and applicationKey strings that are shown to you only once in this page</li> <li>In your FaaSr workflow repo, save the KeyID as a GitHub secret with name <code>S3_ACCESSKEY</code> and save the applicationKey as a GitHub secret with name <code>S3_SECRETKEY</code></li> <li>To use this bucket in your workflows, configure S3 with the Edit Data Store button in the workflow builder as follows:</li> <li><code>Endpoint</code>: enter the endpoint shown for your bucket in Backblaze, including a leading <code>https://</code>. For example: <code>https://s3.us-east-005.backblazeb2.com</code></li> <li><code>Bucket</code>: enter the bucket name you created, e.g. myuniquebucket</li> <li><code>Region</code>: enter the name after s3 in the Endpoint above, e.g. <code>s3.us-east-005</code></li> </ul>"},{"location":"s3/#aws-s3-commercial-cloud-provider","title":"AWS S3 - commercial cloud provider","text":"<p>Follow the documentation on Getting Started with S3</p>"},{"location":"s3/#osn-academic-s3-provider-in-the-us","title":"OSN - academic S3 provider in the US","text":"<ul> <li>For researchers in the US, you can request an allocation of 10+ TB S3 storage.</li> <li>If your request is approved, you will be assigned one S3 bucket, and can then copy the access and secret keys provided to you for use with FaaSr</li> </ul>"},{"location":"security/","title":"Security best-practices","text":"<p>FaaSr faciliattes the process of executing workflows in cloud platforms for you. It is important for you to be aware of best-practices to safeguard from running unintended code as part of these workflows.</p>"},{"location":"security/#use-the-default-container-images-whenever-possible","title":"Use the default container images whenever possible","text":"<p>FaaSr deploys containers for each action of your workflow. The default container images built and published to container registries maintained by FaaSr developers build on Ubuntu Linux, Python, and a minimal set of dependences to minimize the risk of vulnerabilities. For your information, the list of dependences used in FaaSr containers are available in the FaaSr Docker repository.</p> <p>If you need to use a custom container, it is strongly recommended that you build the container yourself, or that you trust the user/registry where the container resides. The FAASR REGISTER GitHub action requires you to explicitly check that you acknowledge the use of custom containers if they are detected in your workflow JSON configuration to prevent you from unintentionally using a custom container.</p>"},{"location":"security/#only-reuse-workflowsfunctions-that-you-trust","title":"Only reuse workflows/functions that you trust","text":"<p>If you plan to reuse workflows (JSON configuration files) and functions (Python or R) created by other users, make sure you trust their provenance. Similarly to when you download and run code from the Internet into your computer, it is crucial that you trust code that runs in your cloud accounts to prevent misuse.</p>"},{"location":"tutorial/","title":"Tutorial","text":"<p>This document guides you through a simple tutorial that uses GitHub Actions and a free S3 data store (Minio Play).</p>"},{"location":"tutorial/#prerequisites","title":"Prerequisites","text":"<p>For this tutorial, all you need is a GitHub account. It is also recommended that your refer to the Setting up the FaaSr-workflow repo documentation for details about the FaaSr-workflow repository.</p>"},{"location":"tutorial/#configure-your-faasr-workflow-repository","title":"Configure your FaaSr-workflow repository","text":"<p>This is a one-time step you go through to set up your workflow repository to host all your workflows, including the tutorial.</p> <ul> <li>Navigate to the FaaSr organization workflow repo</li> <li>Fork the repository to your own GitHub account, keeping the FaaSr-workflow name</li> <li>In your forked repository, click on Actions and click on I understand my workflows, go ahead and enable them to allow FaaSr register/invoke actions to execute</li> </ul>"},{"location":"tutorial/#configure-your-github-pat","title":"Configure your GitHub PAT","text":"<p>You need a personal access token in your repository secrets to run this tutorial.</p> <p>Follow the steps outlined in the GitHub Actions credentials documentation to obtain your PAT. Copy this PAT so you can paste it as a secret in the next step</p>"},{"location":"tutorial/#configure-your-repository-secrets","title":"Configure your repository secrets","text":"<p>Before you can register and invoke workflows, you need to create secrets storing credentials for the cloud providers you will use.</p> <p>Guest S3 Credentials</p> <p>Note that for this tutorial we are using guest credentials for a free S3 data store offered by MinIO. In practice, you will use your own credentials (see the credentials documentation for more information).</p> <ul> <li>In the FaaSr-workflow repo you just forked, click on the Settings tab (top of the page, to the right)</li> <li>Scroll down; on the left pane, click on the pull-down Secrets and variables and select Actions</li> <li>Click on the green New repository secret to enter a new secret</li> <li>Enter the proper Name for each of the three secrets below (one for GitHub actions, two for Minio Play) and past the secret itself in the Secret text box:</li> <li>Click on Add secret and add a secret named <code>GH_PAT</code>, pasting your GitHub PAT created in the previous step</li> <li>Click on Add secret and add a secret named <code>S3_AccessKey</code>, pasting the following text: <code>Q3AM3UQ867SPQQA43P2F</code></li> <li>Click on Add secret and add a secret named <code>S3_SecretKey</code>, pasting the following text: <code>zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG</code></li> </ul>"},{"location":"tutorial/#edit-the-tutorial-json-file","title":"Edit the tutorial JSON file","text":"<p>The tutorial.json file is available in the FaaSr-Functions repository; you will need to import it, configure it with your GitHub account name, and upload to your FaaSr-workflow repository</p>"},{"location":"tutorial/#step-1-import-and-edit-tutorialjson-in-the-web-ui","title":"Step 1: Import and edit tutorial.json in the Web UI","text":"<ul> <li>Open the FaaSr Workflow Builder Web UI in a separate browser window/tab</li> <li>Click on Upload (top left)</li> <li>To import the tutorial file from GitHub, enter in text box: <code>https://github.com/FaaSr/FaaSr-Functions/blob/main/tutorial/tutorial.json</code></li> <li>Click on Import from GitHub URL</li> <li>Click on Edit Compute Servers (top bar), click on GH (top left), then replace YOUR_USERNAME with your GitHub account name</li> </ul>"},{"location":"tutorial/#step-2-download-to-your-computer-and-upload-to-faasr-workflow","title":"Step 2: Download to your computer and upload to FaaSr-workflow","text":"<ul> <li>Click on Download to download tutorial.json to your computer</li> <li>Upload tutorial.json to your FaaSr-workflow repository, using your preferred method (e.g. clicking on the + button, selecting Upload file, and dragging tutorial.json)</li> </ul>"},{"location":"tutorial/#register-the-workflow","title":"Register the workflow","text":"<ul> <li>You first need to register the workflow before it can be invoked</li> <li>Click on Actions</li> <li>Click on <code>(FAASR REGISTER)</code> (left)</li> <li>Click on the <code>Run workflow</code> drop-down; enter <code>tutorial.json</code> and click on <code>Run workflow</code></li> <li>This will take a few seconds to complete; wait until the action completes before proceeding</li> <li>Note: if successful, <code>(FAASR REGISTER)</code> will create two additional GitHub actions for your repository: <code>tutorial-start</code> and <code>tutorial-sum</code>. These are generated automatically for this workflow, which is named <code>tutorial</code> and has two nodes in the graph (<code>start</code> and <code>sum</code>)</li> </ul>"},{"location":"tutorial/#invoke-the-workflow","title":"Invoke the workflow","text":"<p>What code am I running?</p> <p>For this tutorial, function code is sourced from the FaaSr-Functions repository. See Creating Functions for more information on how FaaSr runs function code.</p> <p>Warning</p> <p>Run only workflows and code that you trust with FaaSr. Always check that you trust the provenance of the workflow or code that you are running.</p> <ul> <li>After register, you can invoke the workflow</li> <li>Click on Actions</li> <li>Click on <code>(FAASR INVOKE)</code> (left)</li> <li>Click on the <code>Run workflow</code> drop-down; enter <code>tutorial.json</code> and click on <code>Run workflow</code></li> <li>You will notice three actions will run: <code>(FAASR INVOKE)</code> is used to \"kick-start\" your workflow; <code>tutorial-start</code> and <code>tutorial-sum</code> are the two actions generated automatically for this workflow in the previous step</li> <li>Wait until these actions complete before proceeding</li> </ul>"},{"location":"tutorial/#check-outputs","title":"Check outputs","text":"<p>The workflow outputs are stored in the Minio Play S3 bucket. There are different ways you can access it, with different S3 clients (e.g. Minio client); we will use the Minio Play console in this tutorial:</p> <ul> <li>Browse to minio play web UI</li> <li>For user name, enter <code>Q3AM3UQ867SPQQA43P2F</code></li> <li>For password, enter <code>zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG</code></li> <li>On the search textbox (top left) enter <code>faasr</code>. Select the faasr bucket, then the tutorial folder to see the output files</li> </ul>"},{"location":"tutorial/#another-example","title":"Another example","text":"<p>The tutorial.json workflow is based on two R functions (start, compute_sum). Another example is found in tutorialRpy.json that uses an R function (start) followed by a Python function (compute_sum). If you'd like to test this out, repeat the process of:</p> <ul> <li>importing <code>https://github.com/FaaSr/FaaSr-Functions/blob/main/tutorial/tutorialRpy.json</code> into the WebUI</li> <li>edit your username</li> <li>download tutorialRpy.json to your computer</li> <li>upload tutorilRpy.json to your FaaSr-workflow repo</li> <li><code>(FAASR REGISTER)</code> tutorialRpy.json</li> <li><code>(FAASR INVOKE)</code> tutorialRpy.json</li> </ul>"},{"location":"workflow_builder/","title":"Process for development on workflow builder gui","text":""},{"location":"workflow_builder/#workflow-builder-repository","title":"Workflow-Builder repository","text":"<p>In order to start development on the workflow builder, you must first fork it's repository at github.com/FaaSr/FaaSr-workflow-builder.</p> <ul> <li>Note: Uncheck <code>Copy the main branch only</code> during forking to ensure the gh-pages branch is populated with the most recent build.</li> </ul>"},{"location":"workflow_builder/#local-dev-environment","title":"Local dev environment","text":""},{"location":"workflow_builder/#dependencies","title":"Dependencies","text":"<p>Once you forked this repository, install all necessary dependencies: </p> <ul> <li>Run <code>npm ci</code></li> </ul>"},{"location":"workflow_builder/#testing-locally","title":"Testing Locally","text":"<ul> <li>Run <code>npm start</code> to launch the development server</li> <li>The application will be accessible at localhost:3000/</li> </ul>"},{"location":"workflow_builder/#hosting-on-github-pages","title":"Hosting on GitHub Pages","text":"<p>To host your fork on GitHub Pages for external access:</p> <ul> <li>Run <code>npm run deploy</code> to build and populate the gh-pages branch</li> <li>Navigate to Settings \u2192 Pages in your forked repository</li> <li>Select the <code>gh-pages</code> branch with <code>/root</code> as the source</li> <li>After deployment completes, your site will be available at https://YOUR_GITHUB_USERNAME.github.io/FaaSr-workflow-builder/</li> <li>To deploy updates: run <code>npm run deploy</code> and changes will be live within minutes</li> </ul>"},{"location":"workflow_builder/#applying-changes-to-main-repository","title":"Applying changes to main repository","text":"<p>You simply need to issue a pull request. When the pull request is completed, the page will automaticaly start a GH Action to build what is in the main src branch to the gh-pages branch and then deploy that build at faasr.io/FaaSr-workflow-builder/.</p>"},{"location":"workflow_builder/#internal-development-tools","title":"Internal Development Tools","text":""},{"location":"workflow_builder/#enabling-debug-mode","title":"Enabling debug mode","text":"<p>To access development-only debugging features during local testing:</p> <ul> <li>Set the <code>debug</code> constant in <code>WorkflowContext.js</code> to <code>true</code></li> <li>If you add new debug features, make them depend on this variable as well</li> <li>Remember to set it back to <code>false</code> before submitting a pull request</li> </ul>"},{"location":"workflow_builder/#available-debugging-utilities","title":"Available debugging utilities","text":"<ul> <li>Toggle Workflow: Displays a panel showing the current workflow object</li> <li>Toggle Layout: Displays a panel showing the current nodes and edges objects</li> </ul>"},{"location":"workflow_repo/","title":"Your FaaSr-workflow repo","text":"<p>Your FaaSr-workflow GitHub repository is the default management/launching pad environment for all your workflows; it is the primary location where you will store credentials/secrets, and register and launch workflows. As described in the workflow model document, before you start using FaaSr to run workflows, you need to setup your FaaSr-workflow GitHub repository to hold the following information:</p> <ul> <li>JSON configuration files for each of your workflows</li> <li>GitHub secrets for your compute and data cloud servers</li> <li>GitHub actions that allow you to register and invoke workflows</li> </ul> <p>The simplest way to configure this repository is to fork from the FaaSr organization's FaaSr-workflow repository</p>"},{"location":"workflow_repo/#forking-the-base-repository","title":"Forking the base repository","text":"<ul> <li>In your browser, navigate to the FaaSr-workflow base repository</li> <li>Click on the down arrow next to Fork for a pull-down menu; select Create a new fork</li> <li>Choose your account name as the Owner of the fork</li> <li>While you can choose a different name for your fork, here we assume you leave the default FaaSr-workflow</li> <li>Leave the default Copy the main branch only checkbox checked</li> <li>Click on the green Create fork button</li> </ul>"},{"location":"workflow_repo/#keeping-your-repository-in-sync","title":"Keeping your repository in sync","text":"<p>When the FaaSr-workflow base repository is updated with new features and bug fixes, you will notice that in your own FaaSr-workflow fork there will be a message that <code>This branch is XX commits behind of</code> the upstream <code>FaaSr/FaaSr-workflow:main</code> repo. To incorporate these updates to your fork, use the <code>Sync fork</code> drop-down menu.</p>"},{"location":"workflow_repo/#enabling-faasr-actions","title":"Enabling FaaSr actions","text":"<p>In order to use register and invoke workflows, you also need to perform a one-time configuration to enable running the pre-defined FaaSr register and invoke workflow actions. To do this:</p> <ul> <li>Click on the Actions tab (top of the GitHub page, next to Code and Pull requests)</li> <li>Click on the green button I understand my workflows, please go ahead and enable them</li> </ul>"},{"location":"workflow_repo/#configuring-secrets","title":"Configuring secrets","text":"<p>Before you can register and invoke workflows, you need to create secrets storing credentials for the cloud providers you will use. The following assumes that you already have obtained cloud credentials for those.</p> <ul> <li>In the FaaSr-workflow repo you just forked, click on the Settings tab (top of the page, to the right)</li> <li>Scroll down; on the left pane, click on the pull-down Secrets and variables and select Actions</li> <li>Click on the green New repository secret to enter a new secret</li> <li>Enter the proper Name for your secret (see below) and paste the secret itself in the Secret text box</li> <li>Click on Add secret</li> </ul>"},{"location":"workflow_repo/#naming-convention-for-secrets","title":"Naming convention for secrets","text":""},{"location":"workflow_repo/#s3-data-store-servers","title":"S3 data store servers","text":"<ul> <li>When creating a workflow with the FaaSr Workflow Builder Web UI, you are asked to enter a name for your S3 data server(s)</li> <li>The default compute server name for an S3 server is <code>S3</code></li> <li>Assume the name of a data server you are setting the secrets for is <code>S3</code>, you need two secrets, named exactly as follows (replace <code>S3</code> with the name of the server you configured)</li> <li><code>S3_AccessKey</code></li> <li><code>S3_SecretKey</code></li> <li>The secrets you store under these names are the access and secret keys you obtained from your S3 provider</li> </ul>"},{"location":"workflow_repo/#github-actions","title":"GitHub Actions","text":"<ul> <li>The default compute server name for GitHub Actions is <code>GH</code></li> <li>You should not modify this name unless you plan to use FaaSr with custom advanced actions</li> <li>You need one secret, named <code>GH_PAT</code></li> <li>The secret you store under this name is a GitHub Personal Access Token</li> </ul>"},{"location":"workflow_repo/#aws-lambda","title":"AWS Lambda","text":"<ul> <li>The default compute server name for AWS Lambda is <code>AWS</code></li> <li>You should not modify this name unless you plan to use FaaSr with custom advanced actions</li> <li>You need two secrets, named: <code>AWS_AccessKey</code> and <code>AWS_SecretKey</code></li> <li>The secrets you store under these names are the access and secret keys you obtained from AWS Lambda</li> </ul>"},{"location":"workflow_repo/#google-cloud","title":"Google Cloud","text":"<ul> <li>The default compute server name for AWS Lambda is <code>GCP</code></li> <li>You should not modify this name unless you plan to use FaaSr with custom advanced actions</li> <li>You need one secret named: <code>GCP_SecretKey</code> </li> <li>The secret you store under this name is the secret key you obtained from Google Cloud</li> </ul>"},{"location":"workflow_repo/#openwhisk","title":"OpenWhisk","text":"<ul> <li>The default compute server name for AWS Lambda is <code>OW</code></li> <li>You should not modify this name unless you plan to use FaaSr with custom advanced actions</li> <li>You need one secret named: <code>OW_APIKey</code> </li> <li>The secret you store under this name is the API key you obtained from your OpenWhisk provider</li> </ul>"},{"location":"workflow_repo/#slurm","title":"Slurm","text":"<ul> <li>The default compute server name for AWS Lambda is <code>SLURM</code></li> <li>You should not modify this name unless you plan to use FaaSr with custom advanced actions</li> <li>You need one secret named: <code>SLURM_Token</code> </li> <li>The secret you store under this name is the JWT token you obtained from your Slurm provider</li> </ul>"},{"location":"workflow_repo/#what-next","title":"What next?","text":"<p>It's recommended you start by first running the simple FaaSr tutorial. The secrets you will need are: GH_PAT (your GitHub token), S3_AccessKey and S3_SecretKey (the test Minio Play S3 bucket credentials).</p> <p>Once you are comfortable with the FaaSr tutorial, you may want to check additional FaaSr function examples as helpful workflow templates (JSON files) and functions (R and Python) for you to follow as you develop your own workflows. </p>"},{"location":"workflows/","title":"Creating and Editing Workflows","text":"<p>In a new browser tab or window, open the Workflow Builder Web UI. This Web-based user interface is used for composing and editing workflows to create FaaSr-compliant workflow JSONs. You can create one from scratch, or upload an existing FaaSr workflow JSON and edit it.</p>"},{"location":"workflows/#uploading-and-downloading-workflows","title":"Uploading and Downloading Workflows","text":"<p>To upload an existing workflow, either upload a JSON file stored locally, or enter the URL of a workflow JSON stored in Github (e.g. <code>https://github.com/FaaSr/FaaSr-Functions/blob/main/tutorial/tutorial.json</code>).</p> <p>To download your workflow, navigate to the Download tab, and click 'Download my-workflow.json'. The web UI will verify that the JSON is compliant, and report any errors that need to be fixed, such as missing fields that are required. </p>"},{"location":"workflows/#download-and-save-frequently","title":"Download and save frequently!","text":"<p>It is good practice to download checkpoints of your workflow as you make changes to it in order to prevent loss from a browser crash or unintentional window/tab closing. If the workflow is incomplete, the web UI will notify you of it but still allows you to download.</p>"},{"location":"workflows/#working-with-layout-files","title":"Working with Layout Files","text":"<p>Layout files contain information about the position of the action nodes in the web UI. It does not affect the behavior of the workflow. To save the workflow layout, navigate to the Download tab, and click 'Download my-workflow-layout.json'. To restore a workflow layout, first navigate to the Upload tab and upload your workflow JSON. Then, click 'Load layout file' and upload your layout file to restore the layout of your workflow.</p>"},{"location":"workflows/#secrets","title":"Secrets","text":"<p>The workflow builder web UI is not used to insert secrets into the workflow. Rather, secrets for your data stores and compute servers will be entered as Github Actions Secrets in your FaaSr-workflows repository. See Configuring Secrets in the workflow repo documentation.</p>"},{"location":"workflows/#data-stores","title":"Data Stores","text":"<p>Data Stores are S3 buckets (cloud storage locations), for example AWS S3 or MinIO. In order to run FaaSr workflows, you must specify at least one Data Store. To create a data store, click <code>Create Data Store</code> and specify a name for your data store. This name will be used to reference the data store when using the FaaSr S3 APIs.</p> <p>Endpoint A web URL specifying the location of the S3 server. For example, <code>https://play.min.io</code> for MinIO's Play sandbox, or an Amazon S3 endpoint.</p> <p>Bucket A unique name identifying the S3 bucket. </p> <p>Region For AWS S3, you're required to specify the region of your S3 bucket. For example, <code>us-east-1</code>.</p>"},{"location":"workflows/#compute-servers","title":"Compute Servers","text":"<p>Each action you define in your workflow must have an associated compute server to run on. When you create a new compute server, it will get a default name based on the platform chosen. You should not modify this name unless you plan to use FaaSr with custom advanced actions. You must then fill out any required fields for that platform.</p>"},{"location":"workflows/#actions","title":"Actions","text":"<p>To create a new action in your workflow, navigate to the 'Edit Actions/Functions' tab, and enter a name for the action in the dropdown box. This action name is the unique identifier for a node in the workflow DAG. Then, create a function and link it to this action by supplying the Function Name and Function's Git Repo/Path. It's also required to specify the Function Type (i.e. language, Python and R are supported) and the Compute Server on which the action will be invoked. </p>"},{"location":"workflows/#addingremoving-an-action-from-the-layout","title":"Adding/Removing an Action from the Layout","text":"<p>You may remove an existing action from the layout to exclude it from your workflow without deleting it permanently. If you export a workflow with an action removed from the layout, that action and any incoming or outgoing invocations will be stripped from your workflow. To add an action back to the layout, select that action in the dropdown box, and click 'Add Action to Layout'.</p>"},{"location":"workflows/#next-actions-to-invoke","title":"Next Actions To Invoke","text":"<p>Choose which subsequent actions will be invoked when your action finishes executing. Descendant actions will run when all of their predecessors have finished executing.</p>"},{"location":"workflows/#rank","title":"Rank","text":"<p>If you specify a rank n for a descendant action, that action will be invoked n times. You can set the rank of descendant action by entering the rank in the input next to the descendant's name. Ranked actions may not have more than one predecessor.</p>"},{"location":"workflows/#conditional-invocations","title":"Conditional Invocations","text":"<p>You may also create conditional invocations, which enable branching in your workflow based on the return value of your action. Invocations designated True only activate if the return value of the invoking action is True; likewise, Invocations designated False only activate if the return value of the invoking action is False.</p>"},{"location":"workflows/#action-containers","title":"Action Containers","text":"<p>Each action must be associated with a container that it will run inside of. A FaaSr Container will be automatically selected as a default based on the compute server/function type pair. Advanced users may develop and specify custom containers.</p>"},{"location":"workflows/#packages","title":"Packages","text":"<p>You may specify package for each action; these packages will be included in your container and available for use by your function. PyPI and CRAN packages can be added by name to your functions. GitHub repositories can be added by adding their URL as a GitHub package.</p>"},{"location":"workflows/#workflow-settings","title":"Workflow Settings","text":"<p>In the 'Workflow Settings' tab, you must specify the following settings:</p> <p>Workflow Name Entry Point - The first action in the workflow. The action must not have predecessors. Default Data Store - This data store will be used if you use the FaaSr S3 APIs without specifying a specific data store.</p> <p>You can also change the following optional settings: Log File Name - Logs created in the S3 Bucket will have this name InvocationID - Determines the unique location for a workflow invocation's logs - see Invocation IDs Data Store for Logs - Specify a separate S3 server for logs </p>"}]}